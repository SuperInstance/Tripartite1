//! Embedding Generation and Document Chunking
//!
//! Provides embedding generation for text chunks with intelligent chunking strategies.
//! Supports local models (BGE-Micro-v1.5) and cloud fallback.

use async_trait::async_trait;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tracing::{debug, info};
use unicode_segmentation::UnicodeSegmentation;

use crate::KnowledgeResult;

/// Document type for chunking strategy selection
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DocType {
    /// Code file (will use tree-sitter for syntax-aware chunking)
    Code,
    /// Markdown document (chunk by headings)
    Markdown,
    /// Plain text (sliding window)
    Text,
    /// PDF document (extract text first, then sliding window)
    Pdf,
    /// Auto-detect based on file extension
    Auto,
}

impl DocType {
    /// Detect document type from file path
    pub fn from_path(path: &Path) -> Self {
        match path.extension().and_then(|e| e.to_str()) {
            Some(ext) => match ext {
                "rs" | "py" | "js" | "ts" | "go" | "java" | "cpp" | "c" | "h" | "cs" | "rb"
                | "php" => DocType::Code,
                "md" | "markdown" => DocType::Markdown,
                "pdf" => DocType::Pdf,
                "txt" | "rst" | "adoc" => DocType::Text,
                _ => DocType::Text,
            },
            None => DocType::Text,
        }
    }
}

/// Text chunk with metadata
#[derive(Debug, Clone)]
pub struct Chunk {
    /// Chunk content
    pub content: String,
    /// Starting character offset in original document
    pub start_offset: usize,
    /// Ending character offset in original document
    pub end_offset: usize,
    /// Chunk metadata
    pub metadata: ChunkMetadata,
}

/// Metadata for a chunk
#[derive(Debug, Clone)]
pub struct ChunkMetadata {
    /// Language for code chunks
    pub language: Option<String>,
    /// Heading path for markdown (e.g., ["Introduction", "Getting Started"])
    pub heading_path: Vec<String>,
    /// Chunk index within document
    pub chunk_index: usize,
    /// Total chunks in document
    pub total_chunks: usize,
}

impl Default for ChunkMetadata {
    fn default() -> Self {
        Self {
            language: None,
            heading_path: Vec::new(),
            chunk_index: 0,
            total_chunks: 1,
        }
    }
}

/// Embedding pipeline with chunking strategies
pub struct EmbeddingPipeline {
    embedder: Arc<dyn EmbeddingProvider>,
    chunker: DocumentChunker,
}

impl EmbeddingPipeline {
    /// Create a new embedding pipeline
    pub fn new(model_path: &Path) -> KnowledgeResult<Self> {
        info!(
            "Creating embedding pipeline with model at: {:?}",
            model_path
        );

        // For now, create local embedder (BGE-Micro-v1.5)
        // In production, this would load the actual model from model_path
        let embedder = Arc::new(LocalEmbedder::new("bge-micro"));

        let chunker = DocumentChunker::default();

        Ok(Self { embedder, chunker })
    }

    /// Generate embedding for a single text
    pub async fn embed(&self, text: &str) -> KnowledgeResult<Vec<f32>> {
        self.embedder.embed(text).await
    }

    /// Generate embeddings for multiple texts (batch)
    pub async fn embed_batch(&self, texts: &[&str]) -> KnowledgeResult<Vec<Vec<f32>>> {
        self.embedder.embed_batch(texts).await
    }

    /// Chunk a document into pieces based on its type
    pub fn chunk_document(&self, content: &str, doc_type: DocType) -> Vec<Chunk> {
        self.chunker.chunk(content, doc_type)
    }

    /// Chunk and embed a document in one pass
    pub async fn chunk_and_embed(
        &self,
        content: &str,
        doc_type: DocType,
    ) -> KnowledgeResult<Vec<(Chunk, Vec<f32>)>> {
        let chunks = self.chunk_document(content, doc_type);

        debug!("Chunked document into {} pieces", chunks.len());

        let chunk_texts: Vec<&str> = chunks.iter().map(|c| c.content.as_str()).collect();
        let embeddings = self.embed_batch(&chunk_texts).await?;

        Ok(chunks.into_iter().zip(embeddings).collect())
    }

    /// Get embedding dimensions
    pub fn dimensions(&self) -> u32 {
        self.embedder.dimensions()
    }

    /// Get model name
    pub fn model_name(&self) -> &str {
        self.embedder.model_name()
    }
}

/// Document chunker with multiple strategies
#[derive(Debug, Clone)]
pub struct DocumentChunker {
    /// Maximum words per chunk for sliding window
    pub max_words: usize,
    /// Overlap words for sliding window
    pub overlap_words: usize,
}

impl Default for DocumentChunker {
    fn default() -> Self {
        Self {
            max_words: 500,
            overlap_words: 50,
        }
    }
}

impl DocumentChunker {
    /// Chunk document based on type
    pub fn chunk(&self, content: &str, doc_type: DocType) -> Vec<Chunk> {
        match doc_type {
            DocType::Code => self.chunk_code(content),
            DocType::Markdown => self.chunk_markdown(content),
            DocType::Pdf | DocType::Text => self.chunk_sliding_window(content),
            DocType::Auto => {
                // Try to detect based on content heuristics
                if content.contains("```") || content.contains("fn ") || content.contains("class ")
                {
                    self.chunk_code(content)
                } else if content.starts_with("#") {
                    self.chunk_markdown(content)
                } else {
                    self.chunk_sliding_window(content)
                }
            },
        }
    }

    /// Chunk code file by function/class (placeholder for tree-sitter)
    fn chunk_code(&self, content: &str) -> Vec<Chunk> {
        // TODO: Integrate tree-sitter for syntax-aware chunking
        // For now, use heuristic-based chunking by common patterns

        debug!("Chunking code file (tree-sitter integration pending)");

        let mut chunks = Vec::new();

        // Simple heuristic: split by common function/class markers
        let patterns = [
            "\npub fn ",
            "\nfn ",
            "\npub async fn ",
            "\nasync fn ",
            "\nimpl ",
            "\npub struct ",
            "\nstruct ",
            "\npub enum ",
            "\nenum ",
            "\nclass ",
            "\ndef ",
            "\nfunc ",
        ];

        let mut split_points: Vec<usize> = vec![0];

        for pattern in &patterns {
            let mut offset = 0;
            while let Some(idx) = content[offset..].find(pattern) {
                let abs_idx = offset + idx;
                if !split_points.contains(&abs_idx) {
                    split_points.push(abs_idx);
                }
                offset = abs_idx + pattern.len();
            }
        }

        split_points.sort();

        // Create chunks from split points
        for (i, &start) in split_points.iter().enumerate() {
            let end = if i + 1 < split_points.len() {
                split_points[i + 1]
            } else {
                content.len()
            };

            if end > start {
                let chunk_content = content[start..end].trim().to_string();
                if !chunk_content.is_empty() {
                    chunks.push(Chunk {
                        content: chunk_content,
                        start_offset: start,
                        end_offset: end,
                        metadata: ChunkMetadata {
                            language: Some("code".to_string()),
                            chunk_index: i,
                            total_chunks: split_points.len(),
                            ..Default::default()
                        },
                    });
                }
            }
        }

        // Fallback: if no chunks found, treat as text
        if chunks.is_empty() {
            chunks = self.chunk_sliding_window(content);
        }

        chunks
    }

    /// Chunk markdown by headings
    fn chunk_markdown(&self, content: &str) -> Vec<Chunk> {
        debug!("Chunking markdown by headings");

        let mut chunks = Vec::new();
        let lines: Vec<&str> = content.lines().collect();
        let mut current_chunk = String::new();
        let mut heading_path: Vec<String> = Vec::new();
        let mut start_offset = 0;
        let mut chunk_idx = 0;

        for (line_no, line) in lines.iter().enumerate() {
            // Check for headings
            if line.starts_with("#") {
                // Save previous chunk
                if !current_chunk.trim().is_empty() {
                    chunks.push(Chunk {
                        content: current_chunk.trim().to_string(),
                        start_offset,
                        end_offset: content[..content
                            .lines()
                            .take(line_no)
                            .map(|l| l.len() + 1)
                            .sum::<usize>()]
                            .len(),
                        metadata: ChunkMetadata {
                            heading_path: heading_path.clone(),
                            chunk_index: chunk_idx,
                            total_chunks: 0, // Will update at end
                            ..Default::default()
                        },
                    });
                    chunk_idx += 1;
                }

                // Extract heading level and text
                let level = line.chars().take_while(|&c| c == '#').count();
                let heading_text = line[level..].trim().to_string();

                // Update heading path
                while heading_path.len() >= level {
                    heading_path.pop();
                }
                heading_path.push(heading_text);

                current_chunk = line.to_string();
                start_offset = content
                    .lines()
                    .take(line_no)
                    .map(|l| l.len() + 1)
                    .sum::<usize>();
            } else {
                if current_chunk.is_empty() {
                    start_offset = content
                        .lines()
                        .take(line_no)
                        .map(|l| l.len() + 1)
                        .sum::<usize>();
                }
                current_chunk.push_str(line);
                current_chunk.push('\n');
            }
        }

        // Don't forget the last chunk
        if !current_chunk.trim().is_empty() {
            chunks.push(Chunk {
                content: current_chunk.trim().to_string(),
                start_offset,
                end_offset: content.len(),
                metadata: ChunkMetadata {
                    heading_path: heading_path.clone(),
                    chunk_index: chunk_idx,
                    total_chunks: chunks.len() + 1,
                    ..Default::default()
                },
            });
        }

        // Update total chunks
        let total = chunks.len();
        for chunk in &mut chunks {
            chunk.metadata.total_chunks = total;
        }

        chunks
    }

    /// Chunk text using sliding window
    fn chunk_sliding_window(&self, content: &str) -> Vec<Chunk> {
        debug!(
            "Chunking text with sliding window (max_words={}, overlap={})",
            self.max_words, self.overlap_words
        );

        let mut chunks = Vec::new();

        // Use unicode segmentation for proper word boundaries
        let words: Vec<&str> = content.unicode_words().collect();

        if words.is_empty() {
            return chunks;
        }

        let mut start = 0;
        let mut chunk_idx = 0;

        while start < words.len() {
            let end = (start + self.max_words).min(words.len());
            let chunk_words = &words[start..end];

            // Find character offsets
            let chunk_text = chunk_words.join(" ");
            let start_offset = if start == 0 {
                0
            } else {
                // Find position in original text
                content.find(words[start]).unwrap_or(0)
            };
            let end_offset = start_offset + chunk_text.len();

            chunks.push(Chunk {
                content: chunk_text,
                start_offset,
                end_offset,
                metadata: ChunkMetadata {
                    chunk_index: chunk_idx,
                    total_chunks: 0, // Will update at end
                    ..Default::default()
                },
            });

            // Move window with overlap
            start = end;
            if start < words.len() {
                start = start.saturating_sub(self.overlap_words);
            }

            chunk_idx += 1;
        }

        // Update total chunks
        let total = chunks.len();
        for chunk in &mut chunks {
            chunk.metadata.total_chunks = total;
        }

        chunks
    }
}

/// Trait for embedding providers
#[async_trait]
pub trait EmbeddingProvider: Send + Sync {
    /// Generate embedding for a single text
    async fn embed(&self, text: &str) -> KnowledgeResult<Vec<f32>>;

    /// Generate embeddings for multiple texts (batch)
    async fn embed_batch(&self, texts: &[&str]) -> KnowledgeResult<Vec<Vec<f32>>>;

    /// Get embedding dimensions
    fn dimensions(&self) -> u32;

    /// Get model name
    fn model_name(&self) -> &str;
}

/// Local embedding model using BGE-Micro-v1.5
pub struct LocalEmbedder {
    model_name: String,
    dimensions: u32,
    model_path: PathBuf,
    // TODO: Add actual model handle when integrating with real model
    // model: Option<BgeModel>,
}

impl LocalEmbedder {
    /// Create a new local embedder
    pub fn new(model_name: &str) -> Self {
        // Determine dimensions based on model
        let dimensions = match model_name {
            "bge-micro" | "bge-micro-v1.5" => 384,
            "bge-small" | "bge-small-en-v1.5" => 384,
            "bge-base" | "bge-base-en-v1.5" => 768,
            "bge-large" | "bge-large-en-v1.5" => 1024,
            _ => 384,
        };

        // Default model path (would be ~/.synesis/models/ in production)
        let model_path = PathBuf::from(format!("/.synesis/models/{}", model_name));

        Self {
            model_name: model_name.to_string(),
            dimensions,
            model_path,
        }
    }

    /// Create with custom model path
    pub fn with_path(model_name: &str, model_path: &Path) -> Self {
        let mut embedder = Self::new(model_name);
        embedder.model_path = model_path.to_path_buf();
        embedder
    }

    /// Load the model from disk
    pub async fn load(&mut self) -> KnowledgeResult<()> {
        info!(
            "Loading embedding model '{}' from {:?}",
            self.model_name, self.model_path
        );

        // TODO: Actually load the BGE-Micro-v1.5 model
        // This would integrate with:
        // - candle or burn frameworks for Rust inference
        // - OR llama.cpp bindings
        // - OR Python bridge to sentence-transformers

        // For now, simulate loading
        if !self.model_path.exists() {
            debug!(
                "Model file not found at {:?}, using placeholder mode",
                self.model_path
            );
        }

        Ok(())
    }

    /// Check if model is loaded
    pub fn is_loaded(&self) -> bool {
        // TODO: Return actual model state
        true
    }
}

#[async_trait]
impl EmbeddingProvider for LocalEmbedder {
    async fn embed(&self, text: &str) -> KnowledgeResult<Vec<f32>> {
        debug!("Generating embedding for {} chars", text.len());

        // TODO: Actually generate embedding using BGE-Micro-v1.5 model
        // For now, use deterministic placeholder for testing
        let embedding = generate_placeholder_embedding(text, self.dimensions);

        Ok(embedding)
    }

    async fn embed_batch(&self, texts: &[&str]) -> KnowledgeResult<Vec<Vec<f32>>> {
        debug!("Generating embeddings for {} texts", texts.len());

        let mut results = Vec::with_capacity(texts.len());
        for text in texts {
            results.push(self.embed(text).await?);
        }

        Ok(results)
    }

    fn dimensions(&self) -> u32 {
        self.dimensions
    }

    fn model_name(&self) -> &str {
        &self.model_name
    }
}

/// Generate a placeholder embedding (for testing/development)
/// Uses SHA256 hash to create deterministic embeddings
fn generate_placeholder_embedding(text: &str, dimensions: u32) -> Vec<f32> {
    use sha2::{Digest, Sha256};

    let mut hasher = Sha256::new();
    hasher.update(text.as_bytes());
    let hash = hasher.finalize();

    // Use hash to seed deterministic values
    (0..dimensions)
        .map(|i| {
            let byte_idx = (i as usize) % hash.len();
            let value = hash[byte_idx] as f32 / 255.0;
            // Normalize to [-1, 1]
            (value * 2.0) - 1.0
        })
        .collect()
}

/// Normalize an embedding vector to unit length
pub fn normalize_embedding(embedding: &mut [f32]) {
    let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        for x in embedding.iter_mut() {
            *x /= norm;
        }
    }
}

/// Compute cosine similarity between two embeddings
pub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    if a.len() != b.len() {
        return 0.0;
    }

    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

    if norm_a == 0.0 || norm_b == 0.0 {
        return 0.0;
    }

    dot_product / (norm_a * norm_b)
}

/// Compute euclidean distance between two embeddings
pub fn euclidean_distance(a: &[f32], b: &[f32]) -> f32 {
    if a.len() != b.len() {
        return f32::MAX;
    }

    a.iter()
        .zip(b.iter())
        .map(|(x, y)| (x - y).powi(2))
        .sum::<f32>()
        .sqrt()
}

/// Batch embedding options
#[derive(Debug, Clone)]
pub struct BatchOptions {
    /// Maximum batch size
    pub batch_size: usize,
    /// Whether to show progress
    pub show_progress: bool,
}

impl Default for BatchOptions {
    fn default() -> Self {
        Self {
            batch_size: 32,
            show_progress: true,
        }
    }
}

/// Tree-sitter placeholder module for code chunking
///
/// This module will be used for syntax-aware code chunking in the future.
/// When integrating, add tree-sitter as a dependency and implement:
///
/// ```rust,ignore
/// use tree_sitter::{Parser, Language, Node};
///
/// pub fn chunk_code_with_tree_sitter(
///     content: &str,
///     language: &Language
/// ) -> Vec<Chunk> {
///     let mut parser = Parser::new();
///     parser.set_language(language).unwrap();
///
///     let tree = parser.parse(content, None).unwrap();
///     let root = tree.root_node();
///
///     // Extract function/class definitions
///     extract_semantic_chunks(root, content)
/// }
/// ```
pub mod tree_sitter_placeholder {
    use super::Chunk;

    /// Placeholder for tree-sitter based code chunking
    ///
    /// TODO: Implement actual tree-sitter integration
    /// Required dependencies:
    /// - tree-sitter = "0.20"
    /// - tree-sitter-rust
    /// - tree-sitter-python
    /// - tree-sitter-javascript
    pub fn chunk_code_syntax_aware(_content: &str, _language: &str) -> Vec<Chunk> {
        // This is a placeholder for future implementation
        // When ready, this will:
        // 1. Parse code using tree-sitter
        // 2. Extract function/class definitions
        // 3. Create semantic chunks based on AST
        Vec::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_doc_type_detection() {
        assert_eq!(DocType::from_path(Path::new("test.rs")), DocType::Code);
        assert_eq!(DocType::from_path(Path::new("test.py")), DocType::Code);
        assert_eq!(DocType::from_path(Path::new("test.md")), DocType::Markdown);
        assert_eq!(DocType::from_path(Path::new("test.pdf")), DocType::Pdf);
        assert_eq!(DocType::from_path(Path::new("test.txt")), DocType::Text);
        assert_eq!(DocType::from_path(Path::new("test.unknown")), DocType::Text);
    }

    #[test]
    fn test_markdown_chunking() {
        let content = r#"# Introduction

This is the introduction.

## Getting Started

Here is some content.

### Advanced

More content here."#;

        let chunker = DocumentChunker::default();
        let chunks = chunker.chunk(content, DocType::Markdown);

        assert!(!chunks.is_empty());
        // Should have chunks based on headings
        assert!(chunks.iter().any(|c| c.content.contains("Introduction")));
        assert!(chunks.iter().any(|c| c.content.contains("Getting Started")));
    }

    #[test]
    fn test_sliding_window_chunking() {
        let content = "word ".repeat(600); // 600 words
        let chunker = DocumentChunker::default();
        let chunks = chunker.chunk(&content, DocType::Text);

        // Should create multiple chunks with overlap
        assert!(chunks.len() > 1);

        // First chunk should have max_words or all if less
        assert!(chunks[0].content.split_whitespace().count() <= chunker.max_words);
    }

    #[test]
    fn test_code_chunking() {
        let content = r#"
pub fn hello() {
    println!("Hello");
}

pub struct Foo {
    x: i32,
}

pub fn world() {
    println!("World");
}"#;

        let chunker = DocumentChunker::default();
        let chunks = chunker.chunk(content, DocType::Code);

        // Should find function/class chunks
        assert!(!chunks.is_empty());
    }

    #[test]
    fn test_cosine_similarity() {
        let a = vec![1.0, 0.0, 0.0];
        let b = vec![1.0, 0.0, 0.0];
        assert!((cosine_similarity(&a, &b) - 1.0).abs() < 0.0001);

        let c = vec![0.0, 1.0, 0.0];
        assert!(cosine_similarity(&a, &c).abs() < 0.0001);
    }

    #[test]
    fn test_normalize_embedding() {
        let mut embedding = vec![3.0, 4.0];
        normalize_embedding(&mut embedding);

        // Should be unit length now
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!((norm - 1.0).abs() < 0.0001);
    }

    #[test]
    fn test_placeholder_embedding() {
        let emb1 = generate_placeholder_embedding("hello", 384);
        let emb2 = generate_placeholder_embedding("hello", 384);
        let emb3 = generate_placeholder_embedding("world", 384);

        assert_eq!(emb1.len(), 384);
        assert_eq!(emb1, emb2); // Same text = same embedding
        assert_ne!(emb1, emb3); // Different text = different embedding
    }

    #[tokio::test]
    async fn test_local_embedder() {
        let embedder = LocalEmbedder::new("bge-micro");
        assert_eq!(embedder.dimensions(), 384);

        let embedding = embedder.embed("test text").await.unwrap();
        assert_eq!(embedding.len(), 384);
    }

    #[tokio::test]
    async fn test_embedding_pipeline() {
        let pipeline = EmbeddingPipeline::new(Path::new("/tmp/model")).unwrap();

        // Test chunking
        let chunks = pipeline.chunk_document("test content here", DocType::Text);
        assert!(!chunks.is_empty());

        // Test embedding
        let embedding = pipeline.embed("test").await.unwrap();
        assert_eq!(embedding.len(), 384);
    }

    #[tokio::test]
    async fn test_chunk_and_embed() {
        let content = r#"# Test Document

This is a test document with some content.

It should be chunked properly."#;

        let pipeline = EmbeddingPipeline::new(Path::new("/tmp/model")).unwrap();
        let results = pipeline
            .chunk_and_embed(content, DocType::Markdown)
            .await
            .unwrap();

        assert!(!results.is_empty());
        // Each result should have (chunk, embedding)
        for (chunk, embedding) in results {
            assert!(!chunk.content.is_empty());
            assert_eq!(embedding.len(), 384);
        }
    }

    #[test]
    fn test_chunk_metadata() {
        let chunk = Chunk {
            content: "test".to_string(),
            start_offset: 0,
            end_offset: 4,
            metadata: ChunkMetadata {
                language: Some("rust".to_string()),
                heading_path: vec!["Introduction".to_string()],
                chunk_index: 0,
                total_chunks: 5,
            },
        };

        assert_eq!(chunk.metadata.chunk_index, 0);
        assert_eq!(chunk.metadata.total_chunks, 5);
        assert_eq!(chunk.metadata.language.as_deref(), Some("rust"));
        assert_eq!(chunk.metadata.heading_path.len(), 1);
    }
}
